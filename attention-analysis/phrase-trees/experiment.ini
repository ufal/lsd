[vars]
exp_prefix="/lnet/troja/projects/emnlp18-nonautoregressive/experiments/ende-san_baseline"
data_prefix="/lnet/troja/projects/emnlp18-nonautoregressive/data/ende"
src="en"
tgt="de"

[main]
name="EN -> DE, SAN > split states > CTC"
tf_manager=<tf_manager>
output="{exp_prefix}"
batch_size=40
epochs=10
#train_dataset=<train_data>
#val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>, <att_runner>]
postprocess=None
evaluation=[("target", evaluators.BLEU1), ("target", evaluators.TER), ("target", evaluators.ChrF3), ("target", evaluators.bleu.BLEU)]
logging_period="10m"
validation_period="2h"
runners_batch_size=40
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1
save_n_best=5

[wp_preprocessor]
class=processors.wordpiece.WordpiecePreprocessor
vocabulary=<vocabulary>

#[train_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/{src_train_name}"
#s_target="{data_prefix}/{tgt_train_name}"
#preprocessors=[("source", "source_wp", <wp_preprocessor>), ("target", "target_wp", <wp_preprocessor>)]
#lazy=True

#[val_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/{src_val_name}"
#s_target="{data_prefix}/{tgt_val_name}"
#preprocessors=[("source", "source_wp", <wp_preprocessor>), ("target", "target_wp", <wp_preprocessor>)]

[vocabulary]
class=vocabulary.from_t2t_vocabulary
path="{data_prefix}/wordpieces.txt"

[input_sequence]
class=model.sequence.EmbeddedSequence
vocabulary=<vocabulary>
data_id="source_wp"
embedding_size=512
scale_embeddings_by_depth=True
max_length=64

[encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<input_sequence>
ff_hidden_size=4096
depth=6
n_heads=16
dropout_keep_prob=0.9
attention_dropout_keep_prob=0.9

[decoder]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target_wp"
ff_hidden_size=4096
n_heads_self=16
n_heads_enc=16
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=3.0e-4

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
clip_norm=1.0
optimizer=<adam>

[runner]
class=runners.GreedyRunner
decoder=<decoder>
output_series="target"
postprocess=processors.wordpiece.WordpiecePostprocessor

[att_runner]
class=runners.tensor_runner.TensorRunner
output_series="att"
toplevel_modelpart=<decoder>
toplevel_tensors=[]
tensors_by_name=["encoder/layer_0/self_attention/energies:0","encoder/layer_1/self_attention/energies:0","encoder/layer_2/self_attention/energies:0","encoder/layer_3/self_attention/energies:0","encoder/layer_4/self_attention/energies:0","encoder/layer_5/self_attention/energies:0"]
tensors_by_ref=[]
batch_dims_by_name=[0, 0, 0, 0, 0, 0]
batch_dims_by_ref=[]


