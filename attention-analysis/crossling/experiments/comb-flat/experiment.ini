[vars]
proj_prefix="/home/marecek/projects/lsd/attention-analysis/crossling"
suffix=""
strategy="flat"
dropout=0.6
exp_prefix="{proj_prefix}/experiments/comb-{strategy}"
model_prefix="{proj_prefix}/models"
data_prefix="{proj_prefix}/data"

[main]
name="WMT18 Experiment"
tf_manager=<tf_manager>
output="{model_prefix}/comb-{strategy}"
batch_size=1
epochs=10
#train_dataset=<train_data>
#val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>,<att_runner>]
postprocess=None
evaluation=[("pe", evaluators.BLEU), ("mt", "pe", evaluators.TER), ("pe", evaluators.TER)]
logging_period="4m"
validation_period="30m"
random_seed=356418

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1
save_n_best=5
minimize_metric=True

#[imagenet_reader]
#class=readers.numpy_reader.from_file_list
#prefix="{img_prefix}/flickr30k-images-resnet50"
#suffix=".npz"

#[train_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/train_all.src"
#s_mt="{data_prefix}/train_all.mt"
#s_pe="{data_prefix}/train_all.pe"
#preprocessors=[("source", "source_wp", <wp_prep>), ("mt", "mt_wp", <wp_prep>), ("pe", "pe_wp", <wp_prep>)]
#lazy=True
#
#[val_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/wmt16_dev/dev.mt.nemaprep"
#s_mt="{data_prefix}/wmt16_dev/dev.mt.nemaprep"
#s_pe="{data_prefix}/wmt16_dev/dev.pe.nemaprep"
#preprocessors=[("source", "source_wp", <wp_prep>), ("mt", "mt_wp", <wp_prep>), ("pe", "pe_wp", <wp_prep>)]

[wp_vocab]
class=vocabulary.from_t2t_vocabulary
path="{data_prefix}/wordpieces.txt"

[wp_prep]
class=processors.wordpiece.WordpiecePreprocessor
vocabulary=<wp_vocab>

[source_inpseq]
class=model.sequence.EmbeddedSequence
max_length=60
embedding_size=512
data_id="source_wp"
vocabulary=<wp_vocab>
scale_embeddings_by_depth=True

[source_encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<source_inpseq>
dropout_keep_prob=$dropout
attention_dropout_keep_prob=$dropout
ff_hidden_size=4096
depth=6
n_heads=16

[mt_inpseq]
class=model.sequence.EmbeddedSequence
max_length=60
embedding_size=512
data_id="mt_wp"
vocabulary=<wp_vocab>
scale_embeddings_by_depth=True
embeddings_source=<source_inpseq>

[mt_encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<mt_inpseq>
dropout_keep_prob=$dropout
attention_dropout_keep_prob=$dropout
ff_hidden_size=4096
depth=6
n_heads=16

[decoder]
class=decoders.transformer.TransformerDecoder
encoders=[<source_encoder>,<mt_encoder>]
data_id="pe_wp"
vocabulary=<wp_vocab>
depth=6
n_heads_self=16
n_heads_enc=16
n_heads_hier=4
ff_hidden_size=4096
dropout_keep_prob=$dropout
label_smoothing=0.1
attention_dropout_keep_prob=$dropout
attention_combination_strategy=$strategy
tie_embeddings=True
embeddings_source=<source_inpseq>
max_output_len=60

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
optimizer=<adam>

[decayed_lr]
class=functions.noam_decay
learning_rate=0.2
model_dimension=512
warmup_steps=4000

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=<decayed_lr>

[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="pe"
postprocess=processors.wordpiece.WordpiecePostprocessor

[att_runner]
class=runners.tensor_runner.TensorRunner
output_series="att"
toplevel_modelpart=<decoder>
toplevel_tensors=[]
tensors_by_name=["decoder/layer_0/encdec_attention/energies:0","decoder/layer_1/encdec_attention/energies:0","decoder/layer_2/encdec_attention/energies:0","decoder/layer_3/encdec_attention/energies:0","decoder/layer_4/encdec_attention/energies:0","decoder/layer_5/encdec_attention/energies:0","decoder/layer_0/self_attention/energies:0","decoder/layer_1/self_attention/energies:0","decoder/layer_2/self_attention/energies:0","decoder/layer_3/self_attention/energies:0","decoder/layer_4/self_attention/energies:0","decoder/layer_5/self_attention/energies:0","source_encoder/layer_0/self_attention/energies:0","source_encoder/layer_1/self_attention/energies:0","source_encoder/layer_2/self_attention/energies:0","source_encoder/layer_3/self_attention/energies:0","source_encoder/layer_4/self_attention/energies:0","source_encoder/layer_5/self_attention/energies:0","mt_encoder/layer_0/self_attention/energies:0","mt_encoder/layer_1/self_attention/energies:0","mt_encoder/layer_2/self_attention/energies:0","mt_encoder/layer_3/self_attention/energies:0","mt_encoder/layer_4/self_attention/energies:0","mt_encoder/layer_5/self_attention/energies:0"]
tensors_by_ref=[]
batch_dims_by_name=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
batch_dims_by_ref=[]
