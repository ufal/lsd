[vars]
data_prefix="/net/projects/LSD/naacl2019-data/europarl"
src="en"
tgt1="cs"
tgt2="de"
tgt3="fi"
tgt4="fr"
suffix=""
src_train_name="intersect.{src}.bpe.train"
tgt1_train_name="intersect.{tgt1}.bpe.train"
tgt2_train_name="intersect.{tgt2}.bpe.train"
tgt3_train_name="intersect.{tgt3}.bpe.train"
tgt4_train_name="intersect.{tgt4}.bpe.train"
src_val_name="intersect.{src}.bpe.dev"
tgt_val1_name="intersect.{tgt1}.bpe.dev"
tgt_val2_name="intersect.{tgt2}.bpe.dev"
tgt_val3_name="intersect.{tgt3}.bpe.dev"
tgt_val4_name="intersect.{tgt4}.bpe.dev"
exp_prefix="/net/projects/LSD/naacl2019-data/experiments/{src}{tgt1}{tgt2}{tgt3}{tgt4}_16h"

[main]
name="{src}->{tgt1},{tgt2},{tgt3},{tgt4}, 16 heads"
tf_manager=<tf_manager>
output="{exp_prefix}"
batch_size=40
epochs=10
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>]
postprocess=None
evaluation=[("target", evaluators.BLEU1), ("target", evaluators.TER), ("target", evaluators.ChrF3), ("target", evaluators.bleu.BLEU)]
logging_period="10m"
validation_period="2h"
runners_batch_size=40
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1
save_n_best=5

#[wp_preprocessor]
#class=processors.wordpiece.WordpiecePreprocessor
#vocabulary=<vocabulary>

[train_data]
class=dataset.load_dataset_from_files
s_source="{data_prefix}/{src_train_name}"
s_target1="{data_prefix}/{tgt1_train_name}"
s_target2="{data_prefix}/{tgt2_train_name}"
s_target3="{data_prefix}/{tgt3_train_name}"
s_target4="{data_prefix}/{tgt4_train_name}"
lazy=True

[val_data]
class=dataset.load_dataset_from_files
s_source="{data_prefix}/{src_val_name}"
s_target1="{data_prefix}/{tgt1_val_name}"
s_target2="{data_prefix}/{tgt2_val_name}"
s_target3="{data_prefix}/{tgt3_val_name}"
s_target4="{data_prefix}/{tgt4_val_name}"

[vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["source", "target1", "target2", "target3", "target4"]
max_size=60000
save_file="{exp_prefix}/vocabulary.txt"

[vocabulary1]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["source", "target1"]
max_size=60000
save_file="{exp_prefix}/vocabulary1.txt"

[vocabulary2]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["source", "target2"]
max_size=60000
save_file="{exp_prefix}/vocabulary2.txt"

[vocabulary3]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["source", "target3"]
max_size=60000
save_file="{exp_prefix}/vocabulary3.txt"

[vocabulary4]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["source", "target4"]
max_size=60000
save_file="{exp_prefix}/vocabulary4.txt"

[input_sequence]
class=model.sequence.EmbeddedSequence
vocabulary=<vocabulary>
data_id="source"
embedding_size=512
scale_embeddings_by_depth=True
max_length=64

[encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<input_sequence>
ff_hidden_size=4096
depth=6
n_heads=16
dropout_keep_prob=0.9
attention_dropout_keep_prob=0.9

[decoder1]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary1>
data_id="target1"
ff_hidden_size=4096
n_heads_self=16
n_heads_enc=16
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder2]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary2>
data_id="target2"
ff_hidden_size=4096
n_heads_self=16
n_heads_enc=16
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder3]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary3>
data_id="target3"
ff_hidden_size=4096
n_heads_self=16
n_heads_enc=16
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder4]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary4>
data_id="target4"
ff_hidden_size=4096
n_heads_self=16
n_heads_enc=16
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=3.0e-4

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder1>, <decoder2>, <decoder3>, <decoder4>]
clip_norm=1.0
optimizer=<adam>

[runner]
class=runners.GreedyRunner
decoder=<decoder>
output_series="target"
postprocess=processors.wordpiece.WordpiecePostprocessor

