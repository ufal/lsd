[vars]
data_prefix="/net/projects/LSD/naacl2019-data/europarl"
exp_prefix="/net/projects/LSD/naacl2019-data/experiments"
src="en"
tgt1="cs"
tgt2="de"
tgt3="fi"
tgt4="fr"
numheads=16
tokens="bpe"
vocsize=55000
src_train_name="intersect.{src}.{tokens}.train"
tgt1_train_name="intersect.{tgt1}.{tokens}.train"
tgt2_train_name="intersect.{tgt2}.{tokens}.train"
tgt3_train_name="intersect.{tgt3}.{tokens}.train"
tgt4_train_name="intersect.{tgt4}.{tokens}.train"
src_val_name="intersect.{src}.{tokens}.dev"
tgt1_val_name="intersect.{tgt1}.{tokens}.dev"
tgt2_val_name="intersect.{tgt2}.{tokens}.dev"
tgt3_val_name="intersect.{tgt3}.{tokens}.dev"
tgt4_val_name="intersect.{tgt4}.{tokens}.dev"
exp_name="{src}{tgt1}{tgt2}{tgt3}{tgt4}-{numheads}h-{tokens}"

[main]
name="{src}->{tgt1},{tgt2},{tgt3},{tgt4}, {numheads} heads, using {tokens}"
tf_manager=<tf_manager>
output="{exp_prefix}/{exp_name}"
batch_size=6
epochs=40
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner1>, <runner2>, <runner3>, <runner4>]
postprocess=None
#evaluation=[("target", evaluators.BLEU1), ("target", evaluators.TER), ("target", evaluators.ChrF3), ("target", evaluators.bleu.BLEU)]
evaluation=[("target1", evaluators.BLEU), ("target2", evaluators.BLEU), ("target3", evaluators.BLEU), ("target4", evaluators.BLEU)]
logging_period="10m"
validation_period="2h"
runners_batch_size=40
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1
save_n_best=5

[train_data]
class=dataset.load_dataset_from_files
s_source="{data_prefix}/{src_train_name}"
s_target1="{data_prefix}/{tgt1_train_name}"
s_target2="{data_prefix}/{tgt2_train_name}"
s_target3="{data_prefix}/{tgt3_train_name}"
s_target4="{data_prefix}/{tgt4_train_name}"
lazy=True

[val_data]
class=dataset.load_dataset_from_files
s_source="{data_prefix}/{src_val_name}"
s_target1="{data_prefix}/{tgt1_val_name}"
s_target2="{data_prefix}/{tgt2_val_name}"
s_target3="{data_prefix}/{tgt3_val_name}"
s_target4="{data_prefix}/{tgt4_val_name}"

[vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=["source", "target1", "target2", "target3", "target4"]
max_size=$vocsize
save_file="{exp_prefix}/{exp_name}/vocabulary.txt"

[input_sequence]
class=model.sequence.EmbeddedSequence
vocabulary=<vocabulary>
data_id="source"
embedding_size=512
scale_embeddings_by_depth=True
max_length=64

[encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<input_sequence>
ff_hidden_size=4096
depth=6
n_heads=$numheads
dropout_keep_prob=0.9
attention_dropout_keep_prob=0.9

[decoder1]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target1"
ff_hidden_size=4096
n_heads_self=$numheads
n_heads_enc=$numheads
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder2]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target2"
ff_hidden_size=4096
n_heads_self=$numheads
n_heads_enc=$numheads
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder3]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target3"
ff_hidden_size=4096
n_heads_self=$numheads
n_heads_enc=$numheads
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder4]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target4"
ff_hidden_size=4096
n_heads_self=$numheads
n_heads_enc=$numheads
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=3.0e-4

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder1>, <decoder2>, <decoder3>, <decoder4>]
clip_norm=1.0
optimizer=<adam>

[runner1]
class=runners.GreedyRunner
decoder=<decoder1>
output_series="target1"

[runner2]
class=runners.GreedyRunner
decoder=<decoder2>
output_series="target2"

[runner3]
class=runners.GreedyRunner
decoder=<decoder3>
output_series="target3"

[runner4]
class=runners.GreedyRunner
decoder=<decoder4>
output_series="target4"

