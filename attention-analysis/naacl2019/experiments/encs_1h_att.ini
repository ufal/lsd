[vars]
data_prefix="/net/projects/LSD/naacl2019-data/europarl"
src="en"
tgt="cs"
suffix=""
langpair="{src}{tgt}"
src_train_name="intersect.{src}.bpe.train"
tgt_train_name="intersect.{tgt}.bpe.train"
src_val_name="intersect.{src}.bpe.dev"
tgt_val_name="intersect.{tgt}.bpe.dev"
exp_prefix="/net/projects/LSD/naacl2019-data/experiments/{langpair}_1h"

[main]
name="{src}->{tgt}, 1 head"
tf_manager=<tf_manager>
output="{exp_prefix}"
batch_size=40
epochs=10
#train_dataset=<train_data>
#val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>, <att_runner>]
postprocess=None
evaluation=[("target", evaluators.BLEU1), ("target", evaluators.TER), ("target", evaluators.ChrF3), ("target", evaluators.bleu.BLEU)]
logging_period="10m"
validation_period="2h"
runners_batch_size=40
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1
save_n_best=5

#[wp_preprocessor]
#class=processors.wordpiece.WordpiecePreprocessor
#vocabulary=<vocabulary>

#[train_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/{src_train_name}"
#s_target="{data_prefix}/{tgt_train_name}"
#preprocessors=[("source", "source_wp", <wp_preprocessor>), ("target", "target_wp", <wp_preprocessor>)]
#lazy=True

#[val_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/{src_val_name}"
#s_target="{data_prefix}/{tgt_val_name}"
#preprocessors=[("source", "source_wp", <wp_preprocessor>), ("target", "target_wp", <wp_preprocessor>)]

[vocabulary]
class=vocabulary.from_wordlist
path="{exp_prefix}/vocabulary.txt"

[input_sequence]
class=model.sequence.EmbeddedSequence
vocabulary=<vocabulary>
data_id="source"
embedding_size=512
scale_embeddings_by_depth=True
max_length=64

[encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<input_sequence>
ff_hidden_size=4096
depth=6
n_heads=1
dropout_keep_prob=0.9
attention_dropout_keep_prob=0.9

[decoder]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target"
ff_hidden_size=4096
n_heads_self=16
n_heads_enc=16
depth=6
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=3.0e-4

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
clip_norm=1.0
optimizer=<adam>

[runner]
class=runners.GreedyRunner
decoder=<decoder>
output_series="target"

[att_runner]
class=runners.tensor_runner.TensorRunner
output_series="att"
toplevel_modelpart=<decoder>
toplevel_tensors=[]
tensors_by_name=["encoder/layer_0/self_attention/add:0","encoder/layer_1/self_attention/add:0","encoder/layer_2/self_attention/add:0","encoder/layer_3/self_attention/add:0","encoder/layer_4/self_attention/add:0","encoder/layer_5/self_attention/add:0"]
tensors_by_ref=[]
batch_dims_by_name=[0, 0, 0, 0, 0, 0]
batch_dims_by_ref=[]

