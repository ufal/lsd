[vars]
data_prefix="/net/projects/LSD/naacl2019-data/europarl"
src="en"
tgt1="cs"
tgt2="de"
tgt3="fi"
tgt4="fr"
ff_hidden=4096
depth=6
suffix=""
src_train_name="intersect.{src}.bpe.train"
tgt1_train_name="intersect.{tgt1}.bpe.train"
tgt2_train_name="intersect.{tgt2}.bpe.train"
tgt3_train_name="intersect.{tgt3}.bpe.train"
tgt4_train_name="intersect.{tgt4}.bpe.train"
src_val_name="intersect.{src}.bpe.dev"
tgt1_val_name="intersect.{tgt1}.bpe.dev"
tgt2_val_name="intersect.{tgt2}.bpe.dev"
tgt3_val_name="intersect.{tgt3}.bpe.dev"
tgt4_val_name="intersect.{tgt4}.bpe.dev"
exp_prefix="/net/projects/LSD/naacl2019-data/experiments/{src}{tgt1}{tgt2}{tgt3}{tgt4}_16h"

[main]
name="{src}->{tgt1},{tgt2},{tgt3},{tgt4}, 16 heads"
tf_manager=<tf_manager>
output="{exp_prefix}"
batch_size=10
epochs=10
#train_dataset=<train_data>
#val_dataset=<val_data>
trainer=<trainer>
runners=[<runner1>, <runner2>, <runner3>, <runner4>, <att_runner>]
postprocess=None
#evaluation=[("target", evaluators.BLEU1), ("target", evaluators.TER), ("target", evaluators.ChrF3), ("target", evaluators.bleu.BLEU)]
evaluation=[("target1", evaluators.BLEU1), ("target2", evaluators.BLEU1), ("target3", evaluators.BLEU1), ("target4", evaluators.BLEU1)]
logging_period="10m"
validation_period="2h"
runners_batch_size=10
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1
save_n_best=5

#[wp_preprocessor]
#class=processors.wordpiece.WordpiecePreprocessor
#vocabulary=<vocabulary>

#[train_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/{src_train_name}"
#s_target1="{data_prefix}/{tgt1_train_name}"
#s_target2="{data_prefix}/{tgt2_train_name}"
#s_target3="{data_prefix}/{tgt3_train_name}"
#s_target4="{data_prefix}/{tgt4_train_name}"
#lazy=True

#[val_data]
#class=dataset.load_dataset_from_files
#s_source="{data_prefix}/{src_val_name}"
#s_target1="{data_prefix}/{tgt1_val_name}"
#s_target2="{data_prefix}/{tgt2_val_name}"
#s_target3="{data_prefix}/{tgt3_val_name}"
#s_target4="{data_prefix}/{tgt4_val_name}"

#[vocabulary]
#class=vocabulary.from_dataset
#datasets=[<train_data>]
#series_ids=["source", "target1", "target2", "target3", "target4"]
#max_size=60000
#save_file="{exp_prefix}/vocabulary.txt"

[vocabulary]
class=vocabulary.from_wordlist
path="{exp_prefix}/vocabulary.txt"

[input_sequence]
class=model.sequence.EmbeddedSequence
vocabulary=<vocabulary>
data_id="source"
embedding_size=512
scale_embeddings_by_depth=True
max_length=64

[encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<input_sequence>
ff_hidden_size=$ff_hidden
depth=$depth
n_heads=16
dropout_keep_prob=0.9
attention_dropout_keep_prob=0.9

[decoder1]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target1"
ff_hidden_size=$ff_hidden
n_heads_self=16
n_heads_enc=16
depth=$depth
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder2]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target2"
ff_hidden_size=$ff_hidden
n_heads_self=16
n_heads_enc=16
depth=$depth
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder3]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target3"
ff_hidden_size=$ff_hidden
n_heads_self=16
n_heads_enc=16
depth=$depth
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[decoder4]
class=decoders.transformer.TransformerDecoder
encoder=<encoder>
vocabulary=<vocabulary>
data_id="target4"
ff_hidden_size=$ff_hidden
n_heads_self=16
n_heads_enc=16
depth=$depth
max_output_len=64
dropout_keep_prob=0.9
embeddings_source=<input_sequence>
tie_embeddings=True
label_smoothing=0.1
attention_dropout_keep_prob=0.9

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=3.0e-4

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder1>, <decoder2>, <decoder3>, <decoder4>]
clip_norm=1.0
optimizer=<adam>

[runner1]
class=runners.GreedyRunner
decoder=<decoder1>
output_series="target1"

[runner2]
class=runners.GreedyRunner
decoder=<decoder2>
output_series="target2"

[runner3]
class=runners.GreedyRunner
decoder=<decoder3>
output_series="target3"

[runner4]
class=runners.GreedyRunner
decoder=<decoder4>
output_series="target4"

[att_runner]
class=runners.tensor_runner.TensorRunner
output_series="att"
toplevel_modelpart=<decoder1>
toplevel_tensors=[]
tensors_by_name=["encoder/layer_0/self_attention/add:0","encoder/layer_1/self_attention/add:0","encoder/layer_2/self_attention/add:0","encoder/layer_3/self_attention/add:0","encoder/layer_4/self_attention/add:0","encoder/layer_5/self_attention/add:0"]
tensors_by_ref=[]
batch_dims_by_name=[0, 0, 0, 0, 0, 0]
batch_dims_by_ref=[]

