[vars]
proj_prefix="/home/marecek/projects/lsd/attention-analysis/multilingual"
ordering1=[<encoder_en>, <encoder_de>, <encoder_fr>, <encoder_es>]
ordering2=[<encoder_es>, <encoder_fr>, <encoder_de>, <encoder_en>]
order=$ordering2
strategy="hierarchical"
suffix=""
load_suffix=""
dropout=0.5
heads=8
heads_hier=8
depth=4
depth_dec=6
ff_hidden=2048
model_dim=256
exp_prefix="{proj_prefix}/experiments"
model_prefix="{proj_prefix}/models"
data_prefix="{proj_prefix}/data"

[main]
name="WMT18 Experiment"
output="{model_prefix}/4lang-{strategy}"
batch_size=24
epochs=30
#train_dataset=<train_data>
#val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>,<att_runner>]
postprocess=None
evaluation=[("target", evaluators.BLEU)]
logging_period="1m"
validation_period="8m"
runners_batch_size=128
random_seed=356418

#[train_data]
#class=dataset.load_dataset_from_files
#s_en="{data_prefix}/en.train.nemaprep"
#s_de="{data_prefix}/de.train.nemaprep"
#s_es="{data_prefix}/es.train.nemaprep"
#s_fr="{data_prefix}/fr.train.nemaprep"
#s_target="{data_prefix}/cs.train.nemaprep"
#preprocessors=[("en", "en_wp", <wp_prep>), ("de", "de_wp", <wp_prep>), ("es", "es_wp", <wp_prep>), ("fr", "fr_wp", <wp_prep>), ("target", "target_wp", <wp_prep>)]
#lazy=True

#[val_data]
#class=dataset.load_dataset_from_files
#s_en="{data_prefix}/en.val.nemaprep"
#s_de="{data_prefix}/de.val.nemaprep"
#s_es="{data_prefix}/es.val.nemaprep"
#s_fr="{data_prefix}/fr.val.nemaprep"
#s_target="{data_prefix}/cs.val.nemaprep"
#preprocessors=[("en", "en_wp", <wp_prep>), ("de", "de_wp", <wp_prep>), ("es", "es_wp", <wp_prep>), ("fr", "fr_wp", <wp_prep>), ("target", "target_wp", <wp_prep>)]

[wp_vocab]
class=vocabulary.from_t2t_vocabulary
path="{data_prefix}/wordpieces200.all.txt"

[wp_prep]
class=processors.wordpiece.WordpiecePreprocessor
vocabulary=<wp_vocab>

[inpseq_en]
class=model.sequence.EmbeddedSequence
max_length=50
embedding_size=$model_dim
data_id="en_wp"
vocabulary=<wp_vocab>
scale_embeddings_by_depth=True

[encoder_en]
class=encoders.transformer.TransformerEncoder
input_sequence=<inpseq_en>
dropout_keep_prob=$dropout
attention_dropout_keep_prob=$dropout
ff_hidden_size=$ff_hidden
depth=$depth
n_heads=$heads

[inpseq_es]
class=model.sequence.EmbeddedSequence
max_length=50
embedding_size=$model_dim
data_id="es_wp"
vocabulary=<wp_vocab>
scale_embeddings_by_depth=True
embeddings_source=<inpseq_en>

[encoder_es]
class=encoders.transformer.TransformerEncoder
input_sequence=<inpseq_es>
dropout_keep_prob=$dropout
attention_dropout_keep_prob=$dropout
ff_hidden_size=$ff_hidden
depth=$depth
n_heads=$heads

[inpseq_de]
class=model.sequence.EmbeddedSequence
max_length=50
embedding_size=$model_dim
data_id="de_wp"
vocabulary=<wp_vocab>
scale_embeddings_by_depth=True
embeddings_source=<inpseq_en>

[encoder_de]
class=encoders.transformer.TransformerEncoder
input_sequence=<inpseq_de>
dropout_keep_prob=$dropout
attention_dropout_keep_prob=$dropout
ff_hidden_size=$ff_hidden
depth=$depth
n_heads=$heads

[inpseq_fr]
class=model.sequence.EmbeddedSequence
max_length=50
embedding_size=$model_dim
data_id="fr_wp"
vocabulary=<wp_vocab>
scale_embeddings_by_depth=True
embeddings_source=<inpseq_en>

[encoder_fr]
class=encoders.transformer.TransformerEncoder
input_sequence=<inpseq_fr>
dropout_keep_prob=$dropout
attention_dropout_keep_prob=$dropout
ff_hidden_size=$ff_hidden
depth=$depth
n_heads=$heads

[decoder]
class=decoders.transformer.TransformerDecoder
encoders=$order
data_id="target_wp"
vocabulary=<wp_vocab>
depth=$depth_dec
n_heads_self=$heads
n_heads_enc=$heads
n_heads_hier=$heads_hier
ff_hidden_size=$ff_hidden
dropout_keep_prob=$dropout
label_smoothing=0.1
attention_dropout_keep_prob=$dropout
tie_embeddings=True
embeddings_source=<inpseq_en>
max_output_len=50
attention_combination_strategy=$strategy

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
optimizer=<adam>

[decayed_lr]
class=functions.noam_decay
learning_rate=0.2
model_dimension=$model_dim
warmup_steps=4000

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=<decayed_lr>

[runner]
class=runners.runner.GreedyRunner
decoder=<decoder>
output_series="target"
postprocess=processors.wordpiece.WordpiecePostprocessor

[att_runner]
class=runners.tensor_runner.TensorRunner
output_series="att"
toplevel_modelpart=<decoder>
toplevel_tensors=[]
tensors_by_name=["decoder/layer_0/encdec_attention/enc_hier/energies:0","decoder/layer_1/encdec_attention/enc_hier/energies:0","decoder/layer_2/encdec_attention/enc_hier/energies:0","decoder/layer_3/encdec_attention/enc_hier/energies:0","decoder/layer_4/encdec_attention/enc_hier/energies:0","decoder/layer_5/encdec_attention/enc_hier/energies:0","decoder/layer_0/encdec_attention/enc_0/energies:0","decoder/layer_1/encdec_attention/enc_0/energies:0","decoder/layer_2/encdec_attention/enc_0/energies:0","decoder/layer_3/encdec_attention/enc_0/energies:0","decoder/layer_4/encdec_attention/enc_0/energies:0","decoder/layer_5/encdec_attention/enc_0/energies:0","decoder/layer_0/encdec_attention/enc_1/energies:0","decoder/layer_1/encdec_attention/enc_1/energies:0","decoder/layer_2/encdec_attention/enc_1/energies:0","decoder/layer_3/encdec_attention/enc_1/energies:0","decoder/layer_4/encdec_attention/enc_1/energies:0","decoder/layer_5/encdec_attention/enc_1/energies:0","decoder/layer_0/encdec_attention/enc_2/energies:0","decoder/layer_1/encdec_attention/enc_2/energies:0","decoder/layer_2/encdec_attention/enc_2/energies:0","decoder/layer_3/encdec_attention/enc_2/energies:0","decoder/layer_4/encdec_attention/enc_2/energies:0","decoder/layer_5/encdec_attention/enc_2/energies:0","decoder/layer_0/encdec_attention/enc_3/energies:0","decoder/layer_1/encdec_attention/enc_3/energies:0","decoder/layer_2/encdec_attention/enc_3/energies:0","decoder/layer_3/encdec_attention/enc_3/energies:0","decoder/layer_4/encdec_attention/enc_3/energies:0","decoder/layer_5/encdec_attention/enc_3/energies:0","encoder_en/layer_0/self_attention/energies:0","encoder_en/layer_1/self_attention/energies:0","encoder_en/layer_2/self_attention/energies:0","encoder_en/layer_3/self_attention/energies:0","encoder_en/layer_4/self_attention/energies:0","encoder_en/layer_5/self_attention/energies:0","encoder_es/layer_0/self_attention/energies:0","encoder_es/layer_1/self_attention/energies:0","encoder_es/layer_2/self_attention/energies:0","encoder_es/layer_3/self_attention/energies:0","encoder_es/layer_4/self_attention/energies:0","encoder_es/layer_5/self_attention/energies:0","encoder_de/layer_0/self_attention/energies:0","encoder_de/layer_1/self_attention/energies:0","encoder_de/layer_2/self_attention/energies:0","encoder_de/layer_3/self_attention/energies:0","encoder_de/layer_4/self_attention/energies:0","encoder_de/layer_5/self_attention/energies:0","encoder_fr/layer_0/self_attention/energies:0","encoder_fr/layer_1/self_attention/energies:0","encoder_fr/layer_2/self_attention/energies:0","encoder_fr/layer_3/self_attention/energies:0","encoder_fr/layer_4/self_attention/energies:0","encoder_fr/layer_5/self_attention/energies:0"]
tensors_by_ref=[]
batch_dims_by_name=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
batch_dims_by_ref=[]


