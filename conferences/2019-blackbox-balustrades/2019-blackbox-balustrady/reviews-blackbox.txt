============================================================================ 
BlackboxNLP 2019 Reviews for Submission #31
============================================================================ 

Title: From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions
Authors: David Mareček and Rudolf Rosa
============================================================================
                            REVIEWER #1
============================================================================

---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                   Appropriateness (1-5): 5
                           Clarity (1-5): 4
      Originality / Innovativeness (1-5): 5
           Soundness / Correctness (1-5): 5
             Meaningful Comparison (1-5): 4
                      Thoroughness (1-5): 4
        Impact of Ideas or Results (1-5): 4
                    Recommendation (1-5): 5
               Reviewer Confidence (1-5): 4

Detailed Comments
---------------------------------------------------------------------------
This paper studies syntactic behavior in attention heads for translation-trained transformers, focusing on a specific novel hypothesis derived from manual inspection: The authors propose that 'balustrades', contiguous sequences of tokens for which one attention head attends to a single fixed token, represent something like syntactic constituency. The authors then propose a tidy, conservative experiment to measure this, and find mixed results: These balustrades correspond to ground-truth (PTB/Negra/etc.) constituents more often than we would expect under any simple competing explanation, but they still do not correspond to the ground truth especially closely in absolute terms.

The paper is a classic example of a good workshop paper: A well-done initial analysis of an interesting observation, and it fits this workshop. 

Some suggestions:

– The authors give reason to believe that the MT system they study is reasonably close to the state of the art, but don't explicitly compare its performance to previously published numbers. Doing so would help demonstrate that these findings do actually reflect the behavior of typical current systems.

– It should be straightforward to extend this analysis to self-supervised transformer encoders like GPT or BERT. This could yield new results about another class of models with minimal extra effort.

– The final experiment (Section 6) is not especially informative as-is. I'd encourage the authors to more clearly articulate a hypothesis for this work, and to develop appropriate baselines.
---------------------------------------------------------------------------



============================================================================
                            REVIEWER #2
============================================================================

---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                   Appropriateness (1-5): 5
                           Clarity (1-5): 4
      Originality / Innovativeness (1-5): 4
           Soundness / Correctness (1-5): 4
             Meaningful Comparison (1-5): 3
                      Thoroughness (1-5): 4
        Impact of Ideas or Results (1-5): 4
                    Recommendation (1-5): 4
               Reviewer Confidence (1-5): 4

Detailed Comments
---------------------------------------------------------------------------
The paper presents an attempt to decipher the still-somewhat mysterious self-attention architecture behind models like Transformers by automatically extracting a tree structure from the various attention distributions and measuring their overlap with linguistically-informed phrase-structure trees over the same sentences.

The authors follow an intentionally-linguistically-uninformed tree extraction protocol to highlight the diagnostic nature of their research question. They find that the most naive setup manages to uncover some sensible structure, easily surpassing performance of very simple baselines (e.g., right-adjusted binary trees). The paper also includes interesting analysis and discussion before and after the experiments, lending the reader some useful insights such as the distribution of various patterns found by attention heads, the consistency of attention head patterns across sentences, and types of syntactic connections which are more/less easily detectable using their method. A minimally-informed tweak to the protocol then achieves consistent improvements over the initial setup.

The experiments are done over three closely-related languages with six basic Transformer NMTs as the subject of prodding. It would be interesting to see the behavior in translation on languages unrelated to these, and between unrelated languages. A detial I would appreciate to see is whether the setup used has positional embeddings (I assume so) and how these may affect the findings, since they make adjacent words more structurally-straightforward for Transformers to attend to.
Small typo in l.711 - "reson" should be "reason".
---------------------------------------------------------------------------



============================================================================
                            REVIEWER #3
============================================================================

---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                   Appropriateness (1-5): 5
                           Clarity (1-5): 5
      Originality / Innovativeness (1-5): 3
           Soundness / Correctness (1-5): 4
             Meaningful Comparison (1-5): 5
                      Thoroughness (1-5): 4
        Impact of Ideas or Results (1-5): 3
                    Recommendation (1-5): 4
               Reviewer Confidence (1-5): 4

Detailed Comments
---------------------------------------------------------------------------
The paper gives a qualitative analysis of multi-head attention matrices from NMT systems as well as a quantitative method of decoding parse trees from these matrices. The approach feels complementary to Hewitt & Manning, who decode parse trees from contextual embeddings; the current authors' method differs in that it works from the self-attention matrix and it does not involve any optimization. Performance at decoding trees is modest but better than random trees.

My only technical complaint is this baseline comparison. I know this comparison is similar to what is done in grammar induction, but I think a stronger baseline would be more like the one from Hewitt & Manning, where they compared how well they could decode trees from real embeddings as compared with random embeddings. In this case, the stronger baseline would be the multi-head attention matrix from something like a randomly initialized transformer encoder.
---------------------------------------------------------------------------
